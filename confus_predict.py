
import torch
from torch.utils.data import DataLoader
from datetime import datetime
from fasttext_predict import cfg
from model import Confus
from config import Config
from mydataset import MyDataSet
from sklearn.metrics import confusion_matrix, classification_report
import numpy as np


def read_voc_dict(voc_dict_path):
    voc_dict={}
    dict_list = open(voc_dict_path).readlines()
    for item in dict_list:
        item=item.split(',')
        voc_dict[item[0]] = int(item[1].strip())
    return voc_dict


# 获取编码字典
voc_dict = read_voc_dict('data/dict')

model = Confus(Config())
model.load_state_dict(torch.load('checkpoints/Confus.pth', map_location='cpu'))
model.eval()

def data_process(testFileName):
    """
    将特征数据转换为特征向量
    :param testFileName: 测试数据集
    :param featureDim: 特征维度
    :return: 数据特征向量矩阵，数据标签
    """
    testFile = open(testFileName, "r")
    testLines = testFile.readlines()
    label = []
    feature = []
    for line in testLines:
        per_label, per_data = line.split(',')
        per_feature = per_data.split('\n')[0]
        label.append(per_label)
        feature.append(per_feature)
    testFile.close()
    return feature, label


def predict(seq:str):
    input_idx = []    # 存放序列字母索引，传递给网络编码用的
    seg_list = list(seq)[:1000]   # 获取序列前1000位
    for index, word in enumerate(seg_list):
        if word in voc_dict.keys():
            input_idx.append(voc_dict[word])
        else:
            input_idx.append(voc_dict['<UNK>'])
    input_idx = torch.tensor(input_idx).unsqueeze(dim=0)
    with torch.no_grad():
        logits = model.forward(input_idx)
        predict=torch.softmax(logits[0], dim=1)
        predict = predict.cpu().numpy().reshape(-1)  # 预测结果概率转为numpy
        predict = predict.tolist()  # 预测结果概率转为list
    return predict


if __name__ == '__main__':
    # res = predict('GTGACGCGAGGCGGGTTCTTGGACTGAGTGTGCGGCGCGGTGCGCCGCCTTCCGAGGCTCCTCCCGCGGGTGGCAGCGGACGGGGCGCGCCCCTCGGCCAGTCCTCGGTCCTCAGGCTTGTGGCTCCGTTGAGCACCGGCCGCCGGGCCTCTGGGTCCGTCGAGTGGAGACTCTCTGAAAAGCGTGGGCTCCGTGGCCTCCGGCGCGGCCGCGGCGGGTCGGTCTCCTAGATCATCCGGGAAGCCCACGGGACCCTCAGGCGGGCAGGATGAACGACTGGCACAGGATCTTCACCCAAAACGTGCTTGTCCCTCCCCACCCACAGAGAGCGCGCCAGCCTTGGAAGGAATCCACGGCATTCCAGTGTGTCCTCAAGTGGCTGGACGGACCGGTAATTAGGCAGGGCGTGCTGGAGGTACTGTCAGAGGTTGAATGCCATCTGCGAGTGTCTTTCTTTGATGTCACCTACCGGCACTTCTTTGGGAGGACGTGGAAAACCACAGTGAAGCCGACGAAGAGACCGCCGTCCAGGATCGTCTTTAATGAGCCCTTGTATTTTCACACATCCCTAAACCACCCTCATATCGTGGCTGTGGTGGAAGTGGTCGCTGAGGGCAAGAAACGGGATGGGAGCCTCCAGACATTGTCCTGTGGGTTTGGAATTCTTCGGATCTTCAGCAACCAGCCGGACTCTCCTATCTCTGCTTCCCAGGACAAAAGGTTGCGGCTGTACCATGGCACCCCCAGAGCCCTCCTGCACCCGCTTCTCCAGGACCCCGCAGAGCAAAACAGACACATGACCCTCATTGAGAACTGCAGCCTGCAGTACACGCTGAAGCCACACCCGGCCCTGGAGCCTGCGTTCCACCTTCTTCCTGAGAACCTTCTGGTGTCTGGTCTGCAGCAGATACCTGGCCTGCTTCCAGCTCATGGAGAATCCGGCGACGCTCTCCGAAAGCCTCGCCTCCAGAAGCCCATCACGGGGCACTTGGATGACTTATTCTTCACCCTGTACCCCTCCCTGGAGAAGTTTGAGGAAGAGCTGCTGGAGCTCCACGTCCAGGACCACTTCCAGGAGGGATGTGGCCCACTGGACGGTGGTGCCCTGGAGATCCTGGAGCGGCGCCTGCGTGTGGGCGTGCACAATGGTCTGGGCTTCGTGCAGAGGCCGCAGGTCGTTGTACTGGTGCCTGAGATGGATGTGGCCTTGACGCGCTCAGCTAGCTTCAGCAGGAAAGTGGTCTCCTCTTCCAAGACCAGCTCCGGGAGCCAAGCTCTGGTTTTGAGAAGCCGCCTCCGCCTCCCAGAGATGGTCGGCCACCCTGCATTTGCGGTCATCTTCCAGCTGGAGTACGTGTTCAGCAGCCCTGCAGGAGTGGACGGCAATGCAGCTTCGGTCACCTCTCTGTCCAACCTGGCATGCATGCACATGGTCCGCTGGGCTGTTTGGAACCCCTTGCTGGAAGCTGATTCTGGAAGGGTGACCCTGCCTCTGCAGGGTGGGATCCAGCCCAACCCCTCGCACTGTCTGGTCTACAAGGTACCCTCAGCCAGCATGAGCTCTGAAGAGGTGAAGCAGGTGGAGTCGGGTACACTCCGGTTCCAGTTCTCGCTGGGCTCAGAAGAACACCTGGATGCACCCACGGAGCCTGTCAGTGGCCCCAAAGTGGAGCGGCGGCCTTCCAGGAAACCACCCACGTCCCCTTCGAGCCCGCCAGCGCCAGTACCTCGAGTTCTCGCTGCCCCGCAGAACTCACCTGTGGGACCAGGGTTGTCAATTTCCCAGCTGGCGGCCTCCCCGCGGTCCCCGACTCAGCACTGCTTGGCCAGGCCTACTTCACAGCTACCCCATGGCTCTCAGGCCTCCCCGGCCCAGGCACAGGAGTTCCCGTTGGAGGCCGGTATCTCCCACCTGGAAGCCGACCTGAGCCAGACCTCCCTGGTCCTGGAAACATCCATTGCCGAACAGTTACAGGAGCTGCCGTTCACGCCTTTGCATGCCCCTATTGTTGTGGGAACCCAGACCAGGAGCTCTGCAGGGCAGCCCTCGAGAGCCTCCATGGTGCTCCTGCAGTCCTCCGGCTTTCCCGAGATTCTGGATGCCAATAAACAGCCAGCCGAGGCTGTCAGCGCTACAGAACCTGTGACGTTTAACCCTCAGAAGGAAGAATCAGATTGTCTACAAAGCAACGAGATGGTGCTACAGTTTCTTGCCTTTAGCAGAGTGGCCCAGGACTGCCGAGGAACATCATGGCCAAAGACTGTGTATTTCACCTTCCAGTTCTACCGCTTCCCACCCGCAACGACGCCACGACTGCAGCTGGTCCAGCTGGATGAGGCCGGCCAGCCCAGCTCTGGCGCCCTGACCCACATCCTCGTGCCTGTGAGCAGAGATGGCACCTTTGATGCTGGGTCTCCTGGCTTCCAGCTGAGGTACATGGTGGGCCCTGGGTTCCTGAAGCCAGGTGAGCGGCGCTGCTTTGCCCGCTACCTGGCCGTGCAGACCCTGCAGATTGACGTCTGGGACGGAGACTCCCTGCTGCTCATCGGATCTGCTGCCGTCCAGATGAAGCATCTCCTCCGCCAAGGCCGGCCGGCTGTGCAGGCCTCCCACGAGCTTGAGGTCGTGGCAACTGAATACGAGCAGGACAACATGGTGGTGAGTGGAGACATGCTGGGGTTTGGCCGCGTCAAGCCCATCGGCGTCCACTCGGTGGTGAAGGGCCGGCTGCACCTGACTTTGGCCAACGTGGGTCACCCGTGTGAACAGAAAGTGAGAGGTTGTAGCACATTGCCACCGTCCAGATCTCGGGTCATCTCAAACGATGGAGCCAGCCGCTTCTCTGGAGGCAGCCTCCTCACGACTGGAAGCTCAAGGCGAAAACACGTGGTGCAAGCACAGAAGCTGGCGGACGTGGACAGTGAGCTGGCTGCCATGCTACTGACCCATGCCCGGCAGGGCAAGGGGCCCCAGGACGTCAGCCGCGAGTCGGATGCCACCCGCAGGCGTAAGCTGGAGCGGATGAGGTCTGTGCGCCTGCAGGAGGCCGGGGGAGACTTGGGCCGGCGCGGGACGAGCGTGTTGGCGCAGCAGAGCGTCCGCACACAGCACTTGCGGGACCTACAGGTCATCGCCGCCTACCGGGAACGCACGAAGGCCGAGAGCATCGCCAGCCTGCTGAGCCTGGCCATCACCACGGAGCACACGCTCCACGCCACGCTGGGGGTCGCCGAGTTCTTTGAGTTTGTGCTTAAGAACCCCCACAACACACAGCACACGGTGACTGTGGAGATCGACAACCCCGAGCTCAGCGTCATCGTGGACAGTCAGGAGTGGAGGGACTTCAAGGGTGCTGCTGGCCTGCACACACCGGTGGAGGAGGACATGTTCCACCTGCGTGGCAGCCTGGCCCCCCAGCTCTACCTGCGCCCCCACGAGACCGCCCACGTCCCCTTCAAGTTCCAGAGCTTCTCTGCAGGGCAGCTGGCCATGGTGCAGGCCTCTCCTGGGTTGAGCAACGAGAAGGGCATGGACGCCGTGTCACCTTGGAAGTCCAGCGCAGTGCCCACTAAACACGCCAAGGTCTTGTTCCGAGCGAGTGGTGGCAAGCCCATCGCCGTGCTCTGCCTGACTGTGGAGCTGCAGCCCCACGTGGTGGACCAGGTCTTCCGCTTCTATCACCCGGAGCTCTCCTTCCTGAAGAAGGCCATCCGCCTGCCGCCCTGGCACACATTTCCAGGTGCTCCGGTGGGAATGCTTGGTGAGGACCCCCCAGTCCATGTTCGCTGCAGCGACCCGAACGTCATCTGTGAGACCCAGAATGTGGGCCCCGGGGAACCACGGGACATATTTCTGAAGGTGGCCAGTGGTCCAAGCCCGGAGATCAAAGACTTCTTTGTCATCATTTACTCGGATCGCTGGCTGGCGACACCCACACAGACGTGGCAGGTCTACCTCCACTCCCTGCAGCGCGTGGATGTCTCCTGCGTCGCAGGCCAGCTGACCCGCCTGTCCCTTGTCCTTCGGGGGACACAGACAGTGAGGAAAGTGAGAGCTTTCACCTCTCATCCCCAGGAGCTGAAGACAGACCCCAAAGGTGTCTTCGTGCTGCCGCCTCGTGGGGTGCAGGACCTGCATGTTGGCGTGAGGCCCCTTAGGGCCGGCAGCCGCTTTGTCCATCTCAACCTGGTGGACGTGGATTGCCACCAGCTGGTGGCCTCCTGGCTCGTGTGCCTCTGCTGCCGCCAGCCGCTCATCTCCAAGGCCTTTGAGATCATGTTGGCTGCGGGCGAAGGGAAGGGTGTCAACAAGAGGATCACCTACACCAACCCCTACCCCTCCCGGAGGACATTCCACCTGCACAGCGACCACCCGGAGCTGCTGCGGTTCAGAGAGGACTCCTTCCAGGTCGGGGGTGGAGAGACCTACACCATCGGCTTGCAGTTTGCGCCTAGTCAGAGAGTGGGTGAGGAGGAGATCCTGATCTACATCAATGACCATGAGGACAAAAACGAAGAGGCATTTTGCGTGAAGGTCATCTACCAGTGAGGGCTTGAGGGTGACGTCCTTCCTGCGGCACCCAGCTGGGGCCTGTCTGTGCCCCTCCTGCCCTGCAGGCTGTCCTCCCCGCCTCTCTGCAGCCTTTCACTTCAGTGCCCACCTGGCTGACCTGTGCACTTGGCTGAGGAAGCAGAGACCGAGCGCTGGTCATTTTGTAGTACCTGCATCCAGCTTAGCTGCTGCTGACACCCAGCAGGCCTGGGTTCCGTGAGCGCGAACTCCGTGGTGGTGGGTCTGGCTCTGGTGCTGCCATCTACGCATGTGGGACCCTCGTTATCGCTGTTGCTCAAAATGTATTTTATGAATCATCCTAAATGAGAAAATTATGTTTTTCTTACTGGATTTTGTACAAACATAATCTATTATTTGCTATGCAATATTTTATGCTGGTATTATATCTGTTTTTTAAATTGTTGAACAAAATACTAAACTTTTACACGTCTTCAAAAAAAAAAAAAA')
    # print(res)
    val_dataset = MyDataSet(voc_dict_path='data/dict', data_path='data/test.txt',
                            stop_word_path='data/hit_stopwords.txt')
    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)
    correct_num = 0
    total_num = 0
    pre_label = []
    true_label = []
    with torch.no_grad():
        for batch in val_loader:
            label, data = batch
            label, data = label.to(cfg.device), data.to(cfg.device)
            logits = model.forward(data)
            _, predicted = torch.max(logits[0], dim=1)
            total_num += label.size(0)
            pre_label.append(predicted.item())
            true_label.append(label.item())
            correct_num += (predicted == label).sum().item()
            val_acc = correct_num / total_num
    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
          f" validation accuracy: {val_acc}")
    print("测试集评估:")
    print("混淆矩阵")
    print(confusion_matrix(np.array(true_label), np.array(pre_label)))
    print(classification_report(np.array(true_label), np.array(pre_label), output_dict=True))